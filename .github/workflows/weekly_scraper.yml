name: Amazon Laptop Scraper (Weekly)

on:
  schedule:
    # Runs every Sunday at 4:00 AM UTC
    - cron: '0 4 * * 0'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install playwright playwright-stealth pandas groq
        playwright install chromium
        playwright install-deps chromium

    # STEP 1: Check if we have existing data to analyze
    - name: Check existing data files
      id: check_files
      run: |
        if [ -f "data/weekly.csv" ] && [ -f "data/data.csv" ]; then
          echo "both_exist=true" >> $GITHUB_OUTPUT
          echo "✅ Both weekly.csv and data.csv exist - can analyze"
        elif [ -f "data/data.csv" ]; then
          echo "both_exist=false" >> $GITHUB_OUTPUT
          echo "⚠️ Only data.csv exists - first run, will create weekly.csv"
        else
          echo "both_exist=false" >> $GITHUB_OUTPUT
          echo "⚠️ No data files exist yet"
        fi

    # STEP 2: Run analysis on EXISTING data (last week vs current daily)
    - name: Run Groq Analysis on existing data
      if: steps.check_files.outputs.both_exist == 'true'
      env:
        GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
      run: python analyze_data.py
      
    - name: Check analysis file exists
      id: check_analysis
      run: |
        if [ -f "data/summary.txt" ]; then
          echo "analysis_exists=true" >> $GITHUB_OUTPUT
          echo "✅ Analysis summary created"
        else
          echo "analysis_exists=false" >> $GITHUB_OUTPUT
          echo "⚠️ No analysis summary found"
        fi

    # STEP 3: Send email with summary.txt attachment
    - name: Send analysis email
      if: steps.check_analysis.outputs.analysis_exists == 'true'
      uses: dawidd6/action-send-mail@v3
      with:
        server_address: smtp.gmail.com
        server_port: 587
        username: ${{ secrets.EMAIL_USERNAME }}
        password: ${{ secrets.EMAIL_PASSWORD }}
        subject: Weekly Amazon Laptop Analysis Report - ${{ github.run_id }}
        to: ${{ secrets.CC_EMAIL }}
        from: Amazon Scraper Bot
        body: |
          Hi,
          
          This is your weekly automated analysis report from the Amazon Laptop Scraper.
          
          Please find the detailed analysis summary attached.
          
          This report compares last week's data with today's data to identify trends and changes.
          
          Best regards,
          Amazon Scraper Bot
        attachments: data/summary.txt

    # STEP 4: NOW run the scraper for NEXT week's comparison
    - name: Run scraper for next week
      run: python amazon_scraper.py
        
    # STEP 5: Save new data as weekly.csv
    - name: Update weekly.csv with fresh data
      id: update_weekly
      run: |
        if [ -f "data/data.csv" ]; then
          cp data/data.csv data/weekly.csv
          echo "weekly_updated=true" >> $GITHUB_OUTPUT
          echo "✅ weekly.csv updated with fresh data for next week"
        else
          echo "weekly_updated=false" >> $GITHUB_OUTPUT
          echo "❌ Scraper failed, weekly.csv not updated"
        fi

    # STEP 6: Commit the updated weekly.csv and summary
    - name: Commit updated weekly data
      if: steps.update_weekly.outputs.weekly_updated == 'true'
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        git add data/weekly.csv
        if [ -f "data/summary.txt" ]; then
          git add data/summary.txt
        fi
        git diff --staged --quiet || git commit -m "Update weekly.csv and summary - $(date +'%Y-%m-%d %H:%M')"
        git push
